@article{Latent-Dirichlet-Allocation,
 author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
 title = {Latent Dirichlet Allocation},
 journal = {J. Mach. Learn. Res.},
 issue_date = {3/1/2003},
 volume = {3},
 month = mar,
 year = {2003},
 issn = {1532-4435},
 pages = {993--1022},
 numpages = {30},
 url = {http://dl.acm.org/citation.cfm?id=944919.944937},
 acmid = {944937},
 publisher = {JMLR.org},
}

@ARTICLE{Deerwester90indexingby,
    author = {Scott Deerwester and Susan T. Dumais and George W. Furnas and Thomas K. Landauer and Richard Harshman},
    title = {Indexing by latent semantic analysis},
    journal = {JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE},
    year = {1990},
    volume = {41},
    number = {6},
    pages = {391--407}
}

@inproceedings{clickthrough-based-latent-semantic-models-for-web-search,
author = {Gao, Jianfeng and Toutanova, Kristina and Yih, Scott Wen-tau},
title = {Clickthrough-Based Latent Semantic Models for Web Search},
booktitle = {},
year = {2011},
month = {July},
abstract = {



This paper presents two new document ranking models for Web search based upon the methods of semantic representation and the statistical translation-based approach to information retrieval (IR). Assuming that a query is parallel to the titles of the documents clicked on for that query, large amounts of query-title pairs are constructed from clickthrough data; two latent semantic models are learned from this data. One is a bilingual topic model within the language modeling framework. It ranks documents for a query by the likelihood of the query being a semantics-based translation of the documents. The semantic representation is language independent and learned from query-title pairs, with the assumption that a query and its paired titles share the same distribution over semantic topics. The other is a discriminative projection model within the vector space modeling framework. Unlike Latent Semantic Analysis and its variants, the projection matrix in our model, which is used to map from term vectors into sematic space, is learned discriminatively such that the distance between a query and its paired title, both represented as vectors in the projected semantic space, is smaller than that between the query and the titles of other documents which have no clicks for that query. These models are evaluated on the Web search task using a real world data set. Results show that they significantly outperform their corresponding baseline models, which are state-of-the-art.
},
publisher = {ACM},
url = {https://www.microsoft.com/en-us/research/publication/clickthrough-based-latent-semantic-models-for-web-search/},
address = {},
pages = {},
journal = {},
volume = {},
chapter = {},
isbn = {},
}

@inproceedings{CNNforSemanticRepresentation2014MS,
 author = {Shen, Yelong and He, Xiaodong and Gao, Jianfeng and Deng, Li and Mesnil, Gr{\'e}goire},
 title = {Learning Semantic Representations Using Convolutional Neural Networks for Web Search},
 booktitle = {Proceedings of the 23rd International Conference on World Wide Web},
 series = {WWW '14 Companion},
 year = {2014},
 isbn = {978-1-4503-2745-9},
 location = {Seoul, Korea},
 pages = {373--374},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/2567948.2577348},
 doi = {10.1145/2567948.2577348},
 acmid = {2577348},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {convolutional neural network, semantic representation},
} 

@inproceedings{linguistic-regularities-in-continuous-space-word-representations,
author = {Mikolov, Tomas and Yih, Scott Wen-tau and Zweig, Geoffrey},
title = {Linguistic Regularities in Continuous Space Word Representations},
booktitle = {Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT-2013)},
year = {2013},
month = {May},
abstract = {
Continuous space language models have recently demonstrated outstanding results across a variety of tasks. In this paper, we examine the vector-space word representations that are implicitly learned by the input-layer weights. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, “King - Man + Woman” results in a vector very close to “Queen.” We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40% of the questions. We demonstrate that the word vectors capture semantic regularities by using the vector offset method to answer SemEval-2012 Task 2 questions. Remarkably, this method outperforms the best previous systems.
},
publisher = {Association for Computational Linguistics},
url = {https://www.microsoft.com/en-us/research/publication/linguistic-regularities-in-continuous-space-word-representations/},
address = {},
pages = {},
journal = {},
volume = {},
chapter = {},
isbn = {},
}

@inproceedings{Shokouhi2013,
 author = {Shokouhi, Milad},
 title = {Learning to Personalize Query Auto-completion},
 booktitle = {Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval},
 series = {SIGIR '13},
 year = {2013},
 isbn = {978-1-4503-2034-4},
 location = {Dublin, Ireland},
 pages = {103--112},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2484028.2484076},
 doi = {10.1145/2484028.2484076},
 acmid = {2484076},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {autosuggest, personalized search, query auto-completion},
} 
